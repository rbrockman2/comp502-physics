{\rtf1\ansi\ansicpg1252\deff0
{\fonttbl
{\f0\fnil\fcharset0\fprq0\fttruetype Cambria;}
{\f1\fnil\fcharset0\fprq0\fttruetype Arial;}
{\f2\fnil\fcharset0\fprq0\fttruetype Dingbats;}
{\f3\fnil\fcharset0\fprq0\fttruetype Symbol;}
{\f4\fnil\fcharset0\fprq0\fttruetype Times New Roman;}
{\f5\fnil\fcharset0\fprq0\fttruetype Courier New;}}
{\colortbl
\red0\green0\blue0;
\red255\green255\blue255;
\red255\green255\blue255;}
{\stylesheet
{\s1\fi-431\li720\sbasedon29\snext29 Contents 1;}
{\s2\fi-431\li1440\sbasedon29\snext29 Contents 2;}
{\s3\fi-431\li2160\sbasedon29\snext29 Contents 3;}
{\s8\fi-431\li720\sbasedon29 Lower Roman List;}
{\s5\tx431\sbasedon25\snext29 Numbered Heading 1;}
{\s6\tx431\sbasedon26\snext29 Numbered Heading 2;}
{\s7\fi-431\li720 Square List;}
{\s12\sbasedon29 Endnote Text;}
{\s22\fi-431\li720 Bullet List;}
{\s4\fi-431\li2880\sbasedon29\snext29 Contents 4;}
{\s10\fi-431\li720 Diamond List;}
{\s11\fi-431\li720 Numbered List;}
{\*\cs13\fs20\super Endnote Reference;}
{\s14\fi-431\li720 Triangle List;}
{\s15\tx431\sbasedon27\snext29 Numbered Heading 3;}
{\s16\fi-431\li720 Dashed List;}
{\s17\fi-431\li720\sbasedon11 Upper Roman List;}
{\s18\sb440\sa60\f1\fs24\b\sbasedon29\snext29 Heading 4;}
{\s19\fi-431\li720 Heart List;}
{\s35\fi-431\li720 Box List;}
{\s21\fi-431\li720\sbasedon11 Upper Case List;}
{\s9\fi-288\li288\fs20\sbasedon29 Footnote;}
{\s23\fi-431\li720 Hand List;}
{\s24\fs20\sbasedon29 Footnote Text;}
{\s25\sb440\sa60\f1\fs34\b\sbasedon29\snext29 Heading 1;}
{\s26\sb440\sa60\f1\fs28\b\sbasedon29\snext29 Heading 2;}
{\s20\qc\sb240\sa120\f1\fs32\b\sbasedon29\snext29 Contents Header;}
{\s28\fi-431\li720 Tick List;}
{\s27\sb440\sa60\f1\fs24\b\sbasedon29\snext29 Heading 3;}
{\s30\fi-431\li720\sbasedon11 Lower Case List;}
{\s31\li1440\ri1440\sa120\sbasedon29 Block Text;}
{\s37\f5\sbasedon29 Plain Text;}
{\s33\tx1584\sbasedon5\snext29 Section Heading;}
{\s34\fi-431\li720 Implies List;}
{\s29\f4\fs24 Normal;}
{\s36\fi-431\li720 Star List;}
{\*\cs32\fs20\super Footnote Reference;}
{\s38\tx1584\sbasedon5\snext29 Chapter Heading;}
{\s39\fi-288\li288\sbasedon29 Endnote;}}
\kerning0\cf0\ftnbj\fet2\ftnstart1\ftnnar\aftnnar\ftnstart1\aftnstart1\aenddoc\revprop3{\*\rdf}{\info\uc1}\deftab720\viewkind1\paperw12240\paperh15840\margl1440\margr1440\widowctrl
\sectd\sbknone\colsx0\pgncont\ltrsect
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\b\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch Write-up draft}{\f0\fs24\b\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\b\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch A little bit of particle physics}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab We are searching for supersymmetric }{\f0\fs24\i\lang1033{\*\listtag0}stop squarks}{\f0\fs24\lang1033{\*\listtag0} at the Large Hadron Collider. In the massive data from the LHC, there are collisions of particles which correspond to a particular type of decay mode. The supersymmetric particles decay in this mode, and thus we wish to search for these type of particle-decay events. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab A difficulty is that there is also tremendous background noise, in which we have data that looks very similar to the desired decay mode, but is in fact different. The signal closely resembles the noise. This means our task is to develop a filter subtle enough to distinguish between the two with statistical significance. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch Classification Basics}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab The complexities of particle physics reduce (at least for the purposes of machine learning methods) to a classification problem. There are two target outputs\uc1\u8212\'97that of signal, and that of noise. We could have had one output PE, with signal corresponding to 1 and noise to -1. But in accordance with standard practice, we instead had two output PE\uc1\u8217\'92s, and had signal correspond to [1;0] and noise to [0;1] (appropriately scaled). Thus our network performs a nonlinear mapping from 8-D (or 24-D if we are using the raw data) space to 2-D space. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab Naturally, upon training our network, all outputs will not fall perfectly on [1;0] or [0;1]. Instead, we expect them to typically fall between the two extremes. For an unbiased classification scheme, we simply draw a 45 degree line through the origin, declaring points below to be signal and those above to be noise. However, we can vary this line to improve the ultimate significance ratings. For example, for our purposes false positives are more detrimental than false negatives. For this reason we can bias the classifier so that it is more eager to perform signal or noise classifications. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab Earlier classification methods, such as cuts (i.e., thresholding) used by the physicists, are more simple and primitive than neural network methods. An alternative technique, used by Onkur Sen for his physics senior thesis, is boosted decision trees.  In each case, the ultimate goal is improving significance, because more significance is literally money, because it translates into less time needed at the LHC to achieve an equal certainty of results. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch Artificial Neural Networks}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi960\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch We have input data consisting of signal and noise events, which we wish to classify. For each event, quantities such as energy and momentum four-vectors have been recorded. However, the physicists have devised a different (and smaller) set of scalar parameters, eight in total, which represent significant features of the data. We use these as our input features, and thus we have an eight-dimensional input space for events. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi964\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch Our goal is to maximize the statistical significance, defined as the ratio of signal to the square root of background events. We do this through obtaining a larger ratio of signal to noise than would otherwise be possible without machine learning techniques. We attack the problem with a Multilayer Perceptron trained through Backpropagation, as well as with an SOM. In the case of the SOM, prototypes attached to the lattice neurons naturally find clusters of patterns in the input space. Upon completing the SOM training, we can see which lattice neurons become associated with regions of especially rich signal concentration. We then look to the where in the input space the corresponding prototype points to find a region of higher significance. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch Theoretical limitations of Backpropagation}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch From \uc1\u8220\'93Neural Networks for Pattern Recognition\uc1\u8221\'94, C. Bishop 1995, Oxford University Press, we know that there are theoretical limitations on the usefulness of a multilayer perceptron trained with backpropagation. These considerations help explain the limited usefulness of the MLP in classifying the particle collision data. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch The analysis begins with a reworking of the error measure in the limit of an infinite training data set. In this limit, we can move from finite sums of input patterns to integrals over the distribution of the data in the input space. Bishop\uc1\u8217\'92s analysis considers the conditional averages of the target data (\uc1\u8220\'93conditional\uc1\u8221\'94 here refers to conditional on the given input pattern). \uc0\u8232 }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch With some simplifications (described on p. 202), he achieves the following terse but powerful result: y_k(x,w*) = <t_k | x>. Put into English, this states that the output of the neural network corresponds to the conditional average of the target outputs. Thus, the trained network is essentially returning the conditional mean of the target outputs given some input pattern. This is not an artifact of a particular network architecture, or even the use of a neural network (p. 203). Rather, the importance of the conditional average comes from the training via the sum of squares objective function. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch The relevance of this theory to our project is that certain types of nonlinearity in the distribution of the data will necessarily be ignored by a learning machine trained to minimize a sum of squares error. If, for example, <t_k | x> is an expected value that few (if any) input patterns actually map to (just as the expected value of a die roll is the impossible 3.5), then the network will try to find structure in the conditional average where such structure is not actually present. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch Our data and their corresponding target values may suffer from this sort of nonlinearity. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch SOM Analysis}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch As seen on the second of our results slides, we trained an SOM to find clusters of data points in the input space. We first did with 8-D prototypes attached to each lattice PE. Here the eight dimensions correspond to the physicists\uc1\u8217\'92 derived parameters. After the network was trained, we used three visualization techniques. [By the way, Robert, I never asked you how you did this! The colored SOM pictures are gorgeous.] We counted the number of signal and noise events that mapped to each PE, and we added red or green light, respectively, to each SOM cell accordingly. Thus a bright yellow square has many signal and noise events mapping to that lattice element (as happens in the top-left corner of the derived SOM), and a dark square has few events mapping to it. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch Finally, we also superimposed the prototype on each square (visualized as a blue line, or function). These visualizations combine to give a sense of how the prototypes settled in the input space, and which signal and noise events flow into a given prototype. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch The SOM for the derived variables converged as desired. There are a cluster of lattice cells that have a high concentration of signal events (visualized in red), and much the remaining space is solidly noise (visualized in green). The yellow square in the top-left corner we interpret as an anomaly, but one which in no way detracts from our results. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch The colored plot, however, does not have all of the information that we desire. For example, it leaves out quantitative information on the gain of each PE, where gain is defined to be the ratio of output signal to noise ration (SNR) to the input SNR. In the best case (results slide p. 3), we see a lattice cell with a gain of over 20.  As a theoretical aside, it is important that the gain number be large, because we necessarily sacrifice significance when we cut out many of the input data points. Thus, we need a large concentration of signal in a given lattice cell to compensate for this loss. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch [I skip over results slide 4, because I still do not feel that I have anything convincing to say about this slide. I\uc1\u8217\'92m interested in looking over what is filled in here, as I\uc1\u8217\'92ll analyze all the results much more seriously as we move to publication.]}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch On results slide 5, we repeat the SOM analysis, except this time we use the raw data. Thus, our prototypes extend into 24-D space, corresponding to the 6 particle jets each with their accompanying 4-vector of energy and momentum. We see the increase in input space visually because the blue lines in each cell (representing the prototypes) have more \uc1\u8220\'93wiggles\uc1\u8221\'94. This is because they are connecting 24 points, instead of merely 8. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch Superficially, these results look very similar to the results from the second results slide. In both cases we have a concentration of signal that is desirable. However, the gain is much weaker, as the sixth slide reveals. Where before our gain extended past 20, here it barely reaches 5. This means that our filter is non-trivial. It }{\f0\fs24\i\lang1033{\*\listtag0}does }{\f0\fs24\lang1033{\*\listtag0}succeed in identifying clusters in input space corresponding to signal, but it fails to do this so effectively as to make up for the massive amount of data-points cut out by the filter. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch [Results slide 7 is still puzzling to me. It still feels funny that the physicists\uc1\u8217\'92 method seems to do worse than nothing at all. This suggests that the physicists must at least be aiming for something other than significance as we concieve it, surely? ]  [Actually, this is probably caused by the screwup with the background data Onkur and Padley found on Friday. --RTB2]}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch Also, in the pursuit of publication, we should be very clear on how confident we are in our results, and what results ought to have large error bars around them. For example, if the SOM followed by BP was the best merely by chance, we ought to be very clear about this.] [Actually, we can compute error bars for our significance, based on counting statistics.]}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch General Analysis}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab Ideally, we would want the SOM to perform as well or better on the raw parameters as on the derived parameters. There are several reasons why this might not be the case initially, and why more sophisticated analysis could fix the problem. Firstly, the physicists presumably already have a good intuition for what sorts of variables are the \uc1\u8220\'93defining\uc1\u8221\'94 variables of a system. So by using their derived variables, we are leveraging their intuition. If we make the very reasonable assumption that their intuition is effective, then we reduce the amount of work the SOM must do in finding underlying patterns, because much of the work has already been done. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab But of course, in the bigger picture, we are concerned with improving machines\uc1\u8217\'92 capacity to do data analysis }{\f0\fs24\i\lang1033{\*\listtag0}on their own}{\f0\fs24\lang1033{\*\listtag0}, without the help of physicists\uc1\u8217\'92 intuition. So we should consider a possibility for the relative lack of effectiveness of working with the raw data. }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab A primary cause of this lack of effectiveness is that the derived data includes 3 angles and already normalizes momentum vectors in a certain direction. The raw data does not \uc1\u8230\'85 [I could say more on this, but with Justin\uc1\u8217\'92s work in protein folding, I know this is already very much his area of expertise. So I\uc1\u8217\'92ll stop now and let him take this part entirely]}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch Next Steps}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch In addition to what has already been suggested, perhaps we could also train a neural network to optimize for significance }{\f0\fs24\i\lang1033{\*\listtag0}directly}{\f0\fs24\lang1033{\*\listtag0}, instead of only indirectly via the sum-of-squared-errors objective function? Perhaps we could rig an SOM to also take into account some supervised learning techniques, and thus automatically favor clusters of high significance in its algorithm?}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch I confess that I do not understand the details of Robert\uc1\u8217\'92s backprop code, the extent to which it increases classification accuracy, and exactly how this classification improvement maps into improved significance. Upon understanding this better, I would also have more to say on alternative neural networks with significance as an objective function already baked in. [RTB2 - My BP code is not special, except for the \uc1\u8220\'93bias\uc1\u8221\'94 aspect, which is just a cruder version of what you were trying with the parabola code.]}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\fi720\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\ul\lang1033{\*\listtag0}\abinodiroverride\ltrch Additional Notes}{\f0\fs24\ul\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab }{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab What happened to the double boxplot image that showed the statistical distributions of the input data? Was it decided that the image was good, but that it simply did not make the cut for the presentation, or was there some flaw in it that I am unaware of? [It was cut for time, but should be included in the report. --RTB2]}{\f0\fs24\lang1033{\*\listtag0}\par}
\pard\plain\ltrpar\ql\ri720\sl360\slmult1\itap0{\f0\fs24\lang1033{\*\listtag0}\abinodiroverride\ltrch \tab If it turns out that the image was good, we could include it plus a similar image that works with the 24 scalar inputs for the raw data. [If we decide to do this, let me know, and I\uc1\u8217\'92ll generalize the code I wrote for the 8-D case].  [This is a good idea. --RTB2]}{\f0\fs24\lang1033{\*\listtag0}\par}}