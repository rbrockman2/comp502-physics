{\rtf1\ansi\ansicpg1252\cocoartf1187\cocoasubrtf340
{\fonttbl\f0\fnil\fcharset0 Cambria;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\ri720\sl360\slmult1

\f0\b\fs24 \cf0 \ul \ulc0 Write-up draft\
\pard\pardeftab720\ri720\sl360\slmult1

\b0 \cf0 \ulnone \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ul \ulc0 A little bit of particle physics\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ulnone \
	We are searching for supersymmetric top quarks at the Large Hadron Collider. In the massive data from the LHC, there are collisions of particles which correspond to a particular type of decay mode. The supersymmetric particles decay in this mode, and thus we wish to search for these type of particle-decay events. \
\
	A difficulty is that there is also tremendous background noise, in which we have data that looks very similar to the desired decay mode, but is in fact different. The signal closely resembles the noise. This means our task is to develop a filter subtle enough to distinguish between the two with statistical significance. \
\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ul \ulc0 Classification Basics\
\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ulnone 	The complexities of particle physics reduce (at least for the purposes of machine learning methods) to a classification problem. There are two target outputs\'97that of signal, and that of noise. We could have had one output PE, with signal corresponding to 1 and noise to -1. But in accordance with standard practice, we instead had two output PE\'92s, and had signal correspond to [1;0] and noise to [0;1] (appropriately scaled). Thus our network performs a nonlinear mapping from 8-D (or 24-D if we are using the raw data) space to 2-D space. \
\
	Naturally, upon training our network, all outputs will not fall perfectly on [1;0] or [0;1]. Instead, we expect them to typically fall between the two extremes. For an unbiased classification scheme, we simply draw a 45 degree line through the origin, declaring points below to be signal and those above to be noise. However, we can vary this line to improve the ultimate significance ratings. For example, for our purposes false positives are more detrimental than false negatives. For this reason we can bias the classifier so that it is more eager to perform signal or noise classifications. \
	\
	Earlier classification methods, such as cuts (i.e., thresholding) used by the physicists, are more simple and primitive than neural network methods. An alternative technique, used by Onkur Sen for his physics senior thesis, is boosted decision trees.  In each case, the ultimate goal is improving significance, because more significance is literally money, because it translates into less time needed at the LHC to achieve an equal certainty of results. \
\
\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ul \ulc0 Artificial Neural Networks\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ulnone \
\pard\pardeftab720\fi960\ri720\sl360\slmult1
\cf0 We have input data consisting of signal and noise events, which we wish to classify. For each event, quantities such as energy and momentum four-vectors have been recorded. However, the physicists have devised a different (and smaller) set of scalar parameters, eight in total, which represent significant features of the data. We use these as our input features, and thus we have an eight-dimensional input space for events. \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \
\pard\pardeftab720\fi965\ri720\sl360\slmult1
\cf0 Our goal is to maximize the statistical significance, defined as the ratio of signal to the square root of background events. We do this through obtaining a larger ratio of signal to noise than would otherwise be possible without machine learning techniques. We attack the problem with a Multilayer Perceptron trained through Backpropagation, as well as with an SOM. In the case of the SOM, prototypes attached to the lattice neurons naturally find clusters of patterns in the input space. Upon completing the SOM training, we can see which lattice neurons become associated with regions of especially rich signal concentration. We then look to the where in the input space the corresponding prototype points to find a region of higher significance. \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \
\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ul \ulc0 Theoretical limitations of Backpropagation\
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ulnone \
\pard\pardeftab720\fi720\ri720\sl360\slmult1
\cf0 From \'93Neural Networks for Pattern Recognition\'94, C. Bishop 1995, Oxford University Press, we know that there are theoretical limitations on the usefulness of a multilayer perceptron trained with backpropagation. These considerations help explain the limited usefulness of the MLP in classifying the particle collision data. \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \
\pard\pardeftab720\fi720\ri720\sl360\slmult1
\cf0 The analysis begins with a reworking of the error measure in the limit of an infinite training data set. In this limit, we can move from finite sums of input patterns to integrals over the distribution of the data in the input space. Bishop\'92s analysis considers the conditional averages of the target data (\'93conditional\'94 here refers to conditional on the given input pattern). \uc0\u8232 \
With some simplifications (described on p. 202), he achieves the following terse but powerful result: y_k(x,w*) = <t_k | x>. Put into English, this states that the output of the neural network corresponds to the conditional average of the target outputs. Thus, the trained network is essentially returning the conditional mean of the target outputs given some input pattern. This is not an artifact of a particular network architecture, or even the use of a neural network (p. 203). Rather, the importance of the conditional average comes from the training via the sum of squares objective function. \
\
The relevance of this theory to our project is that certain types of nonlinearity in the distribution of the data will necessarily be ignored by a learning machine trained to minimize a sum of squares error. If, for example, <t_k | x> is an expected value that few (if any) input patterns actually map to (just as the expected value of a die roll is the impossible 3.5), then the network will try to find structure in the conditional average where such structure is not actually present. \
\
Our data and their corresponding target values may suffer from this sort of nonlinearity. \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \ul \ulc0 SOM Analysis\
\
\pard\pardeftab720\fi720\ri720\sl360\slmult1
\cf0 \ulnone As seen on the second of our results slides, we trained an SOM to find clusters of data points in the input space. We first did with 8-D prototypes attached to each lattice PE. Here the eight dimensions correspond to the physicists\'92 derived parameters. After the network was trained, we used three visualization techniques. [By the way, Robert, I never asked you how you did this! The colored SOM pictures are gorgeous.] We counted the number of signal and noise events that mapped to each PE, and we added red or green light, respectively, to each SOM cell accordingly. Thus a bright yellow square has many signal and noise events mapping to that lattice element (as happens in the top-left corner of the derived SOM), and a dark square has few events mapping to it. \
Finally, we also superimposed the prototype on each square (visualized as a blue line, or function). These visualizations combine to give a sense of how the prototypes settled in the input space, and which signal and noise events flow into a given prototype. \
\pard\pardeftab720\ri720\sl360\slmult1
\cf0 \
\
}