#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article-beamer
\begin_preamble
\usepackage{color}
\usepackage{lipsum}
\definecolor{RiceBlue}{cmyk}{1,0.93,0.28,0.22}
\definecolor{RiceGray}{cmyk}{0,0,0,0.77}
\usetheme{Singapore} 
\usecolortheme[named=RiceGray]{structure}
%\usetheme{Luebeck}
%\usecolortheme{beaver}
\usepackage{textpos}

\setbeamercolor{titlelike}{fg=RiceBlue}

\setbeamercovered{transparent}
% or whatever (possibly just delete it)

%\pgfdeclareimage[height=1cm]{institution-logo}{RiceLogo_TMCMYK300DPI.jpg}
%\logo{\includegraphics[height=7mm]{RiceLogo_TMCMYK300DPI.jpg}\vspace{220pt}}
%\logo{\includegraphics[height=1cm]{RiceLogo_TMCMYK300DPI.jpg}}
%\addtobeamertemplate{frametitle}{}{%
%\begin{textblock*}{100mm}(-.93cm,7.57cm)
%\includegraphics[height=.75cm]{figures/RiceLogo_TMCMYK300DPI.jpg}
%\includegraphics[height=.4cm]{RUTypeTM(Blue)CMYK300DPI.jpg}
%\tiny Department of Electrical Engineering - Rice University - April 18, 2013
%\end{textblock*}}

\setbeamertemplate{caption}[numbered]
\end_preamble
\options compress,serif
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex8
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize letterpaper
\use_geometry true
\use_amsmath 2
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine natbib_numerical
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{titlepage}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{gobble}
\end_layout

\end_inset


\end_layout

\begin_layout Title
Using Artificial Neural Networks in the 
\emph on
stop squark 
\emph default
Search
\end_layout

\begin_layout Author
R.
 Brockman, J.
\begin_inset space ~
\end_inset

DeVito, and R.
 LeVan
\end_layout

\begin_layout Institute
Department of Electrical Engineering
\begin_inset Newline newline
\end_inset

Rice University 
\end_layout

\begin_layout Date
ELEC 502, Spring 2013
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
Table of Work
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Percentage
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Robert
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Justin
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ricky
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The following causes the table of contents to be shown at the beginning
 of every subsection.
 Delete this, if you do not want it.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%Statement of Problem
\end_layout

\begin_layout Plain Layout

%Objectives
\end_layout

\begin_layout Plain Layout

%Technical Approach
\end_layout

\begin_layout Plain Layout

%Results
\end_layout

\begin_layout Plain Layout

%Appendices
\end_layout

\begin_layout Plain Layout


\backslash
end{titlepage}
\end_layout

\end_inset


\end_layout

\begin_layout BeginFrame
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
huge 
\end_layout

\end_inset

Outline
\begin_inset ERT
status open

\begin_layout Plain Layout

 }
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{gobble}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{arabic}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Statement of Problem
\end_layout

\begin_layout Standard
With the discovery of the Higgs-like particle at the Large Hadron Collider
 (LHC), all of the important predictions of the Standard Model of particle
 physics have been tested.
 The focus of particle physics has now shifted to the search for new particles
 associated with extensions of the Standard Model.
 One such search involves the search for a particular particle associated
 with an extension to the Standard Model known as supersymmetry (SUSY).
 In particular, physicists at the Compact Muon Solenoid (CMS) detector group
 at the LHC are interested in verifying a SUSY model in which specific p-p
 collision events generate the super-symmetric partner to the top quark
 known as the 
\emph on
stop
\emph default
 squark.
 Verification of these models requires an efficient classifier capable of
 separating signal 
\emph on
stop
\emph default
 squark events from background events.
 More efficient classifiers allow discoveries of fundamental particles to
 be made with fewer events, less expensive beam time, and thus lower cost.
 
\end_layout

\begin_layout Section
Objectives
\end_layout

\begin_layout Standard
Our main objective for this project was to use Artificial Neural Networks
 to build a classifier for distinguishing two types of collision events.
 One class of event indicates the presence of supersymmetric particles (specific
ally, a 
\emph on
stop squark
\emph default
), and the other class of event represents a troublesome background event.
\end_layout

\begin_layout Standard
Our goal is to make a classifier that is better and more efficient at separating
 the signal from the background than models currently being developed and
 used.
 Another method being developed by theoretical particle physicists is a
 method using a series of cuts on a set of eight derived parameters.
 
\begin_inset CommandInset citation
LatexCommand citep
key ":Dutta2012"

\end_inset

 The efficiency of the classifiers will be compared by measuring the discovery
 significance per unit beam luminosity.
\end_layout

\begin_layout Standard
Onkur Sen, a physics senior working with Dr.
 Paul Padley at Bonner Lab here at Rice, is attempting to improve upon this
 with a boosted decision tree algorithm, using the same derived parameters.
 Dr.
 Padley also provided invaluable assistance with understanding the underlying
 physics.
\end_layout

\begin_layout Section
Technical Approach
\end_layout

\begin_layout Standard
Our classifier will be trained on simulated data generated by PYTHIA, a
 particle physics simulator which can be tuned to represent the appropriate
 SUSY model.
 If the SUSY theory under consideration is correct, a classifier which works
 on the simulated data will detect the 
\emph on
stop 
\emph default
squark on the real data from the CMS detector.
 
\end_layout

\begin_layout Subsection
Physics of the Signal Events
\end_layout

\begin_layout Standard
The event type we are trying to detect is shown below:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
p+p & \rightarrow\tilde{t}+\bar{\tilde{t}}\\
\tilde{t} & \rightarrow t+\widetilde{\chi}_{1}^{0}\\
\bar{\tilde{t}} & \rightarrow\bar{t}+\widetilde{\chi}_{1}^{0}\\
t & \rightarrow b+j+j\\
\bar{t} & \rightarrow\bar{b}+j+j
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Here components of two protons colliding roughly head on down the accelerator
 beam axis combine to form two 
\emph on
stop squarks
\emph default
, represented by 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\tilde{t}$
\end_inset

 and 
\begin_inset Formula $\bar{\tilde{t}}$
\end_inset

.
 (The bar on top represents an antiparticle.) The stop squarks decay into
 top quarks and neutralinos.
 Neutralinos escape the experiment undetected, carrying missing energy with
 them.
 In the signal event, the top quarks then decay into three other quarks,
 one of which is a b quark and the other two are lighter quarks from the
 1st two particle generations.
 The quarks form recognizable jets of particles in the detector, and the
 jets from the b quarks can be differentiated from the others.
 Thus the detector will see this event as 6 jets, 2 of which are from b
 quarks, and there will be considerable missing transverse energy caused
 by the escape of the neutralinos.
 (The total transverse momentum is the vector sum of all of the momenta
 perpendicular to the beam axis of the detected particles .
 If it is not zero, momentum conservation demands that there be undetected
 particles which have escaped with the missing momentum.) This is shown in
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Schematic-of-the"

\end_inset

.
\end_layout

\begin_layout Standard
The relevant raw numbers from this event are therefore the energy-momentum
 four-vectors for the four normal jets (j) and two b jets (b), for a total
 of 24 scalars per event.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/Schematic_Signal_Noise.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Schematic-of-the"

\end_inset

Schematic of the Signal and Noise events 
\begin_inset CommandInset citation
LatexCommand citep
key ":Dutta2012"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Physics of the Background Events
\end_layout

\begin_layout Standard
Unfortunately, there is a troublesome background event that looks very similar
 to the signal event, but contains no interesting new physics:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
p+p & \rightarrow t+\bar{t}+j+j\\
t & \rightarrow b+j+j\\
\bar{t} & \rightarrow\bar{b}+W^{-}\\
W^{-} & \rightarrow e^{-}+\nu_{e}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Here the initial collision produces two top quarks directly as well as two
 low-mass quarks which form jets.
 One of the top quarks decays into a b quark and two low mass quarks as
 in the signal event.
 The other top quark decays into a b quark, a lepton (electron or muon)
 and a neutrino which carries off missing energy.
 The big problem is that lepton detection is not perfect, and when the lepton
 is missed, this event also appears as 6 jets, 2 of which are from b quarks,
 and with missing transverse energy, just like the signal event.
 Our task is to study the four-vectors of the 6 jets in each of the event
 types to see if we can distinguish them.
\end_layout

\begin_layout Subsection
Derived Scalars - Physics Perspective
\end_layout

\begin_layout Standard
Dutta, et.
 al.
 have derived 8 scalar features to help in identifying the background event
 
\begin_inset CommandInset citation
LatexCommand citep
key ":Dutta2012"

\end_inset

.
 The basic strategy comes from the observation that two of the jets from
 low-mass quarks (hereafter 
\emph on
j
\emph default
) emerge directly from the initial collision in the background event, whereas
 in the signal event the 
\emph on
j
\emph default
's are paired and each pair is associated with a b quark.
 Thus, the patterns in the angles between jets and the energy of the jet
 groupings should be different.
 
\end_layout

\begin_layout Standard
Python scripts provided by Onkur Sen compute the scalar features for each
 event and output them in a format which we can feed into a classifier.
 The scripts group the quark jets from each event into two groups of 3 jets
 each, referred to as System A and System B.
 Each group contains one b quark and 2 normal jets.
 The scripts then compute the following:
\end_layout

\begin_layout Itemize
M3 : The invariant mass of all three jets in a group.
 This scalar does not change with reference frame, and should correspond
 to the rest mass of the top quark that spawned the jets.
 This is computed for both groups, so we have M3A and M3B.
\end_layout

\begin_layout Itemize
M2: The invariant mass of the two non-b jets in a group.
 This should correspond to the difference between the top and b quark rest
 masses in the signal case.
 In the background case, two non-b jets emerged from the initial collision
 and thus there should be less correlation.
 This is computed for both groups, so we have M2A and M2B.
\end_layout

\begin_layout Itemize
B_ANGLES: The azimuthal angle for the b quark in System B.
\end_layout

\begin_layout Itemize
J1_ANGLES: The azimuthal angle for the 1st non-b quark in System B.
\end_layout

\begin_layout Itemize
J2_ANGLES: The azimuthal angle for the 2nd non-b quark in System B.
\end_layout

\begin_layout Itemize
MISSING_E: The missing transverse energy.
\end_layout

\begin_layout Subsection
Derived Scalars - Machine Learning Perspective
\end_layout

\begin_layout Standard
The data for we plan to classify with an ANN thus consists of vectors of
 8 real-valued scalars, 5 of which are energies in GeV (range 0-1000, mostly
 around 200) and 3 of which are angles in radians (range 0-
\begin_inset Formula $\pi$
\end_inset

).
 Each of the vectors is labeled as signal or background, so we have only
 2 classes.
 Right now we have 7595 signal events and 18292 background events generated
 by Onkur’s scripts.
 We have enough data to split both signal and noise evenly into training,
 cross-validation, and test sets.
 The cross-validation sets are used to determine the correct filter strength
 for the SOMs, as well as to tune the output cutoff threshold for signal
 and noise for backpropagation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/derived_stats_plot_8.eps
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Distribution of Scaled 8-D Input Values
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/raw_stats_plot_24.eps
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Distribution of Scaled 24-D Raw Data Values
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Classification Basics
\end_layout

\begin_layout Standard
The complexities of particle physics reduce (at least for the purposes of
 machine learning methods) to a classification problem.
 There are two target outputs—that of signal, and that of noise.
 We could have had one output PE, with signal corresponding to 1 and noise
 to -1.
 But in accordance with standard practice, we instead had two output PE’s,
 and had signal correspond to [1;0] and noise to [0;1] (appropriately scaled).
 Thus our network performs a nonlinear mapping from 8-D (or 24-D if we are
 using the raw data) space to 2-D space.
 From the simulated data, with the help of Onkur's code, we were able to
 create a length 24 vector of raw data for each signal or background event
 and a length 8 vector of derived variables from the raw data 
\begin_inset CommandInset citation
LatexCommand citep
key ":Dutta2012"

\end_inset


\end_layout

\begin_layout Standard
Naturally, upon training our network, all outputs will not fall perfectly
 on [1;0] or [0;1].
 Instead, we expect them to typically fall between the two extremes.
 For an unbiased classification scheme, we simply draw a 45 degree line
 through the origin, declaring points below to be signal and those above
 to be noise.
 However, we can vary this line to improve the ultimate significance ratings.
 For example, for our purposes false positives are more detrimental than
 false negatives.
 For this reason we can bias the classifier so that it is more eager to
 perform signal or noise classifications.
 
\end_layout

\begin_layout Standard
Earlier classification methods, such as cuts (i.e., thresholding as outlined
 in Dutta, et al.
 
\begin_inset CommandInset citation
LatexCommand citep
key ":Dutta2012"

\end_inset

 ) used by the physicists, are more simple and primitive than neural network
 methods.
 An alternative technique, used by Onkur Sen for his physics senior thesis,
 is boosted decision trees.
 In each case, the ultimate goal is improving significance, because more
 significance is extremely valuable because it translates into less time
 needed at the LHC to achieve an equal certainty of results.
 
\end_layout

\begin_layout Subsection
Significance
\end_layout

\begin_layout Standard
A measure of discovery confidence based on a certain number of high energy
 particle collisions used commonly in physics is significance.
 Every significance measure in the Results Section is based on a standard
 number of collisions.
 Significance is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mbox{significance}=\frac{N_{\mbox{Signal}}}{\sqrt{N_{\mbox{Background}}}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $N_{\mbox{Signal}}:=$
\end_inset

 number of signal events that come through the filter and
\begin_inset Formula $N_{\mbox{Background}}:=$
\end_inset

 number of noise events that come through the filter.
\end_layout

\begin_layout Standard
Our goal is to maximize the statistical significance, defined as the ratio
 of signal to the square root of background events.
 We do this through obtaining a larger ratio of signal to noise than would
 otherwise be possible without machine learning techniques.
 We attack the problem with a Multilayer Perceptron trained through Backpropagat
ion, as well as with an SOM.
 In the case of the SOM, prototypes attached to the lattice neurons naturally
 find clusters of patterns in the input space.
 Upon completing the SOM training, we can see which lattice neurons become
 associated with regions of especially rich signal concentration.
 We then look to the where in the input space the corresponding prototype
 points to find a region of higher significance.
 
\end_layout

\begin_layout Subsection
Multilayer Perceptron with Back-Propagation
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset space \qquad{}
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\backslash
small
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="17" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Architecture
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Topology
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(8 +
\begin_inset Formula $1_{Bias}$
\end_inset

) - (30 +
\begin_inset Formula $1_{Bias}$
\end_inset

) - 2
\begin_inset Formula $_{output}$
\end_inset

 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Transfer Function
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\tanh$
\end_inset

 with slope 
\begin_inset Formula $b=1$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Learning Parameters
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Initial weights
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w\sim U[-0.1,\,0.1]$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Learning rate, 
\begin_inset Formula $\gamma(t)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma(t)=0.01(1-0.0001)^{t}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Momentum, 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha=0.3$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Epoch size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $K=1$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Stopping criteria
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
learning step > 100,000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Error measure (Err)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RMSE 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Monitoring frequency (m)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1,000 Learning Steps
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Input/Output Scaling
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Input Scaling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(-0.9,0.9)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Output Scaling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(-0.9,0.9)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Performance Evaluation
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy measure (
\begin_inset Formula $\mbox{Acc}_{X}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Significance = 
\begin_inset Formula $\frac{S}{\sqrt{B}}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset space \qquad{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Back-Propagation Filter Settings
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
SOM
\end_layout

\begin_layout Standard
\align left
As seen on the second of our results slides, we trained an SOM to find clusters
 of data points in the input space.
 We first did with 8-D prototypes attached to each lattice PE.
 Here the eight dimensions correspond to the physicists’ derived parameters.
 After the network was trained, we used three visualization techniques.
 We counted the number of signal and noise events that mapped to each PE,
 and we added red or green light, respectively, to each SOM cell accordingly.
 Thus a bright yellow square has many signal and noise events mapping to
 that lattice element (as happens in the top-left corner of the derived
 SOM), and a dark square has few events mapping to it.
 Finally, we also superimposed the prototype on each square (visualized
 as a blue line, or function).
 These visualizations combine to give a sense of how the prototypes settled
 in the input space, and which signal and noise events flow into a given
 prototype.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset space \qquad{}
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\backslash
small
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="15" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Architecture
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Topology
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10 x 10
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Learning Parameters
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Initial weights
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w\sim U[-0.1,\,0.1]$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Learning rate, 
\begin_inset Formula $\gamma(t)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma(t)=0.3(1-0.00001)^{t}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Neighborhood, 
\begin_inset Formula $\sigma(t)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma(t)=1.5+3.5\left(1-0.00001\right)^{t}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Epoch size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $K=1$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Stopping criteria
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
learning step > 750,000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Monitoring frequency (m)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1,000 Learning Steps
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Input/Output Scaling
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Input Scaling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Angles in Degrees, Otherwise None
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Output Scaling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
None
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size large
\noun on
Performance Evaluation
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy measure (
\begin_inset Formula $\mbox{Acc}_{X}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Significance = 
\begin_inset Formula $\frac{S}{\sqrt{B}}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset space \qquad{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
SOM Filter Settings
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Summary of Significance
\end_layout

\begin_layout Standard
As seen in the table below (Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Table-of-the"

\end_inset

), the physicists' methods of thresholding are actually counter-effective
 in terms of significance.
 Although they do improve classification, they cut out so many events that
 significance remains small.
 With back-propagation, we created an effective filter that was able to
 classify enough events as signal to still generate substantial significance.
 The same holds true for the SOM trained on the derived variables.
 In the richest lattice PE, the gain was sufficiently high to compensate
 for the other events in the input space which were cut out.
 Finally, combining different learning paradigms gives us the best results
 of all.
 We achieve these results by feeding the signal-rich input space (as determined
 by our SOM) into a back-propagation network.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series bold
\shape smallcaps
\size large
Method
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series bold
\shape smallcaps
\size large
Test Set
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series bold
\shape smallcaps
\size large
Significance
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\size large
\color blue
Thresholding 
\begin_inset CommandInset citation
LatexCommand citep
key ":Dutta2012"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\size large
\color blue
1.93
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
No Filter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.62
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
Back-Propagation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.79
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
Self-Organizing Map for Derived Variables
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.69
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\size large
\color red
SOM then Back-Propagation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\size large
\color red
4.36
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
Self-Organizing Map for Raw Data
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.62
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Table-of-the"

\end_inset

Table of the significance values produced using each method
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Back-Propagation Results
\end_layout

\begin_layout Standard
As shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Back-Propagation-Results-Derived"

\end_inset

, training an MLP with back-propagation results in steadily increasing significa
nce, up to 
\begin_inset Formula $10^{5}$
\end_inset

 epochs.
 The back-propagation network itself is being trained under the objective
 function of sum-of-squared-error.
 However, this indirectly results in improved significance.
 As seen in the figure, significance also increases, as desired, when the
 cross-validation data is run through the network taught with the training
 data.
 In this case, significance happens to be even higher for the cross-validation
 data, but this is simply an artifact of the cross-validation data having
 more signal events to begin with.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../RobertBPcode/SigTrainHist_BP.eps
	height 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Back-Propagation-Results-Derived"

\end_inset

Back-Propagation Results for Derived Variables
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Self-Organizing Map
\end_layout

\begin_layout Standard
The SOM for the derived variables converged as shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SOM-Derived"

\end_inset

.
 There are a cluster of lattice cells that have a high concentration of
 signal events (visualized in red), and much the remaining space is solidly
 noise (visualized in green).
 The yellow square in the top-left corner we interpret as an anomaly, but
 one which does not seem to detract from our results.
 The 
\emph on
gain
\emph default
 for each PE, defined to be the output signal to noise ratio (SNR) divided
 by the input SNR, is shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SOM-Derived-Gain"

\end_inset

.
 Note that the highest gain for a lattice cell is over 20.
 It is important that the gain be large, because we necessarily sacrifice
 significance when we cut out many of the input data points.
 Thus, we need a large concentration of signal in a given lattice cell to
 compensate for this loss.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../RobertSOMcode/PrototypesGraph_8.eps
	height 70text%

\end_inset


\begin_inset Graphics
	filename ../presentation/figures/RGLegend.png
	width 20text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SOM-Derived"

\end_inset

SOM Signal and Noise Density plots with weights for the Derived Variables
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../RobertSOMcode/GainGraph_8.eps
	height 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SOM-Derived-Gain"

\end_inset

SOM Normalized Signal to Noise Ratios for the Derived Variables
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SOM-Raw"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SOM-Raw-Gain"

\end_inset

, we repeat the SOM analysis, except this time we use the raw data.
 Thus, our prototypes extend into 24-D space, corresponding to the 6 particle
 jets each with their accompanying 4-vector of energy and momentum.
 We see the increase in input space visually because the blue lines in each
 cell (representing the prototypes) have more “wiggles”.
 This is because they are connecting 24 points, instead of merely 8.
 Superficially, these results look very similar to the results from the
 8 derived inputs case, however, the gain is much weaker.
 Where before our gain extended past 20, here it barely reaches 5.
 Although this SOM does succeed in identifying signal clusters in input
 space, it fails to do this effectively enough to make up for the large
 number of data-points cut out by the filter.
 This filter does not increase the significance at all.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../RobertSOMcode/exemplarSquaresPlot_24.eps
	height 70text%

\end_inset


\begin_inset Graphics
	filename ../presentation/figures/RGLegend.png
	width 20text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SOM-Raw"

\end_inset

SOM Signal and Noise Density plots with weights for the Raw Data 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../RobertSOMcode/gain_24.eps
	height 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SOM-Raw-Gain"

\end_inset

SOM Normalized Signal to Noise Ratios for the Raw Data 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align left
Ideally, we would want the SOM to perform as well or better on the raw parameter
s as on the derived parameters.
 There are several reasons why this might not be the case initially, and
 why more sophisticated analysis could fix the problem.
 Firstly, the physicists presumably already have a good intuition for what
 sorts of variables are the “defining” variables of a system.
 So by using their derived variables, we are leveraging their intuition.
 If we make the very reasonable assumption that their intuition is effective,
 then we reduce the amount of work the SOM must do in finding underlying
 patterns, because much of the work has already been done.
 
\end_layout

\begin_layout Standard
\align left
A primary cause of this lack of effectiveness is that the derived variables
 include 3 angles and already normalizes momentum vectors regardless of
 direction.
 These angles were independent of the absolute direction of the resulting
 quark jets, whereas the 24 raw data variables depended on the absolute
 angular direction of the jets.
 For the purposes of the analysis from a physics perspective, an event where
 the detected jets are just a rotation of the jets of another event should
 be considered the same event.
 However, the neural networks examined are unable to perform this rotation
 automatically by themselves (which is not expected, either).
 In continuing this analysis we need to attempt to align the x, y, and z
 momenta of the jets in order to make sure similar events are classified
 together by the neural network.
 We believe that this will produce even better results as we cannot be sure
 the 8 derived variables take into account all of the data of the 24 raw
 data variables.
\end_layout

\begin_layout Subsection
Two-Stage BP-SOM Classifier Results
\end_layout

\begin_layout Standard
Although our attempt to implement a two stage classifier by applying backpropaga
tion to the output of the SOM filter did seem to increase the significance
 compared to SOM or backpropagation alone, this may have been a result of
 chance.
 As seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SOM-followed-by"

\end_inset

, the neural network did not train well, with no meaningful increase in
 the significance on the training data.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../RobertBPcode/SigTrainHist_BP_SOM_8.eps
	height 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SOM-followed-by"

\end_inset

SOM followed by BP for Derived Variables
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Next Steps
\end_layout

\begin_layout Standard
With our main results acquired, we look to how we can improve our results,
 with the aim of publication in mind.
 First, as described earlier, we would like to better align the 24 non-derived
 variables to take into account angle variation.
 Second, we would like to experiment further with the training parameters
 in our learning machines.
 Perhaps other combinations are more optimal than those used so far.
 Third, we would like to run a second SOM only on the SOM cells of the first
 SOM with a high gain.
 In this manner we hope to further improve significance with an even higher
 gain.
 
\end_layout

\begin_layout Standard
Finally, we would like to develop an alternative learning machine that would
 use significance directly as an objective function.
 In our current system, the MLP seeks to minimize sum-of-squared-errors,
 and this indirectly benefits significance.
 Likewise, the SOM simply tries to find clusters in the input space.
 This improves significance to the extent that certain clusters are signal-rich.
 Perhaps we could train an SOM superimposed with supervised learning methods,
 causing the SOM to favor clusters of signal, especially large clusters
 that will maintain high significance.
 Another option is to use an LVQ, an already existing supervised learning
 paradigm.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout Section
\start_of_appendix
Theoretical Limitations of Backpropagation
\end_layout

\begin_layout Standard
From Bishop, et al.
 
\begin_inset CommandInset citation
LatexCommand citep
key ":Bishop1995"

\end_inset

, we know that there are theoretical limitations on the usefulness of a
 multilayer perceptron trained with backpropagation.
 These considerations help explain the limited usefulness of the MLP in
 classifying the particle collision data.
 
\end_layout

\begin_layout Standard
The analysis begins with a reworking of the error measure in the limit of
 an infinite training data set.
 In this limit, we can move from finite sums of input patterns to integrals
 over the distribution of the data in the input space.
 Bishops analysis considers the conditional averages of the target data
 (conditional here refers to conditional on the given input pattern).
 With some simplifications (described on p.
 202), he achieves the following terse but powerful result: y_k(x,w*) =
 <t_k x>.
 Put into English, this states that the output of the neural network corresponds
 to the conditional average of the target outputs.
 Thus, the trained network is essentially returning the conditional mean
 of the target outputs given some input pattern.
 This is not an artifact of a particular network architecture, or even the
 use of a neural network (p.
 203).
 Rather, the importance of the conditional average comes from the training
 via the sum of squares objective function.
 
\end_layout

\begin_layout Standard
The relevance of this theory to our project is that certain types of nonlinearit
y in the distribution of the data will necessarily be ignored by a learning
 machine trained to minimize a sum of squares error.
 If, for example, <t_k | x> is an expected value that few (if any) input
 patterns actually map to (just as the expected value of a die roll is the
 impossible 3.5), then the network will try to find structure in the conditional
 average where such structure is not actually present.
 
\end_layout

\begin_layout Standard
Our data and their corresponding target values may suffer from this sort
 of nonlinearity.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../elec502proj"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
